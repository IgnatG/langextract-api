# ── Application ──────────────────────────────────────────────────────────────
APP_NAME=LangCore API
API_V1_STR=/api/v1
ROOT_PATH=
DEBUG=false
LOG_LEVEL=info

# ── CORS (JSON array) ───────────────────────────────────────────────────────
CORS_ORIGINS=["*"]

# ── Redis ────────────────────────────────────────────────────────────────────
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# ── API Keys (populate for production) ───────────────────────────────────────
OPENAI_API_KEY=
GEMINI_API_KEY=
ANTHROPIC_API_KEY=
MISTRAL_API_KEY=
LANGCORE_API_KEY=

# ── Local / Self-Hosted LLMs ─────────────────────────────────────────────────
# Ollama base URL (only needed when running Ollama; default http://localhost:11434)
# OLLAMA_API_BASE=http://localhost:11434
# ── Provider ─────────────────────────────────────────────────────────────────
DEFAULT_PROVIDER=gpt-4o
# ── LangCore defaults ────────────────────────────────────────────────────
DEFAULT_MAX_WORKERS=10
DEFAULT_MAX_CHAR_BUFFER=1000
# ── LLM Response Cache (via litellm + Redis) ────────────────────────────────
# Set to false to disable LLM response caching
EXTRACTION_CACHE_ENABLED=true
# TTL in seconds for cached responses (default 24 h)
EXTRACTION_CACHE_TTL=86400
# Cache backend: redis (default, cross-worker), disk (dev/offline), none
EXTRACTION_CACHE_BACKEND=redis
# Directory for disk backend (only used when EXTRACTION_CACHE_BACKEND=disk)
# EXTRACTION_CACHE_DIR=.extraction_cache
# ── Task defaults ────────────────────────────────────────────────────────────
TASK_TIME_LIMIT=3600
TASK_SOFT_TIME_LIMIT=3300
RESULT_EXPIRES=86400

# ── Audit logging (langcore-audit) ────────────────────────────────────────
# Enable structured audit logging for every LLM inference call
AUDIT_ENABLED=true
# Sink type: logging (stdlib), jsonfile (NDJSON file), otel (OpenTelemetry)
AUDIT_SINK=logging
# Path for the NDJSON audit file (only used when AUDIT_SINK=jsonfile)
AUDIT_LOG_PATH=audit.jsonl
# Truncated sample length for prompt/response in audit records (unset = disabled)
# AUDIT_SAMPLE_LENGTH=200

# ── Guardrails / output validation (langcore-guardrails) ──────────────────
# Enable LLM output validation with automatic retry & corrective prompting
GUARDRAILS_ENABLED=true
# Maximum retry attempts when validation fails (default 3)
GUARDRAILS_MAX_RETRIES=3
# Include invalid output in correction prompt (set false to save tokens)
GUARDRAILS_INCLUDE_OUTPUT_IN_CORRECTION=true
# Truncate original prompt in correction prompts (unset = no limit)
# GUARDRAILS_MAX_CORRECTION_PROMPT_LENGTH=2000
# Truncate invalid output in correction prompts (unset = no limit)
# GUARDRAILS_MAX_CORRECTION_OUTPUT_LENGTH=1000
