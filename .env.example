# ── Application ──────────────────────────────────────────────────────────────
APP_NAME=LangCore API
API_V1_STR=/api/v1
ROOT_PATH=
DEBUG=false
LOG_LEVEL=info

# ── CORS (JSON array) ───────────────────────────────────────────────────────
CORS_ORIGINS=["*"]

# ── Redis ────────────────────────────────────────────────────────────────────
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_DB=0

# ── API Keys (populate for production) ───────────────────────────────────────
OPENAI_API_KEY=
GEMINI_API_KEY=
ANTHROPIC_API_KEY=
MISTRAL_API_KEY=
LANGCORE_API_KEY=

# ── Local / Self-Hosted LLMs ─────────────────────────────────────────────────
# Ollama base URL (only needed when running Ollama; default http://localhost:11434)
# OLLAMA_API_BASE=http://localhost:11434
# ── Document download security ────────────────────────────────────────────
# Comma-separated list of domains allowed for document_url downloads.
# Leave empty to allow all domains. Include your Supabase project domain
# when using signed URLs for anonymised text files.
# Example: ALLOWED_URL_DOMAINS=myproject.supabase.co,storage.googleapis.com
ALLOWED_URL_DOMAINS=

# Comma-separated hostnames exempt from SSRF checks (blocked hostname +
# private IP). Use for local dev when callback_url needs localhost or
# host.docker.internal.
# Example: SSRF_EXEMPT_HOSTNAMES=localhost,host.docker.internal
SSRF_EXEMPT_HOSTNAMES=localhost,host.docker.internal

# ── Provider ─────────────────────────────────────────────────────────────────
DEFAULT_PROVIDER=gpt-4o
# ── LangCore defaults ────────────────────────────────────────────────────
DEFAULT_MAX_WORKERS=10
DEFAULT_MAX_CHAR_BUFFER=1000
# ── LLM Response Cache (via litellm + Redis) ────────────────────────────────
# Set to false to disable LLM response caching
EXTRACTION_CACHE_ENABLED=true
# TTL in seconds for cached responses (default 24 h)
EXTRACTION_CACHE_TTL=86400
# Cache backend: redis (default, cross-worker), disk (dev/offline), none
EXTRACTION_CACHE_BACKEND=redis
# Directory for disk backend (only used when EXTRACTION_CACHE_BACKEND=disk)
# EXTRACTION_CACHE_DIR=.extraction_cache
# ── Task defaults ────────────────────────────────────────────────────────────
TASK_TIME_LIMIT=3600
TASK_SOFT_TIME_LIMIT=3300
RESULT_EXPIRES=86400

# ── Audit logging (langcore-audit) ────────────────────────────────────────
# Enable structured audit logging for every LLM inference call
AUDIT_ENABLED=true
# Sink type: logging (stdlib), jsonfile (NDJSON file), otel (OpenTelemetry)
AUDIT_SINK=logging
# Path for the NDJSON audit file (only used when AUDIT_SINK=jsonfile)
AUDIT_LOG_PATH=audit.jsonl
# Truncated sample length for prompt/response in audit records (unset = disabled)
# AUDIT_SAMPLE_LENGTH=200

# ── Guardrails / output validation (langcore-guardrails) ──────────────────
# Enable LLM output validation with automatic retry & corrective prompting
GUARDRAILS_ENABLED=true
# Maximum retry attempts when validation fails (default 3)
GUARDRAILS_MAX_RETRIES=3
# Include invalid output in correction prompt (set false to save tokens)
GUARDRAILS_INCLUDE_OUTPUT_IN_CORRECTION=true
# Truncate original prompt in correction prompts (unset = no limit)
# GUARDRAILS_MAX_CORRECTION_PROMPT_LENGTH=2000
# Truncate invalid output in correction prompts (unset = no limit)
# GUARDRAILS_MAX_CORRECTION_OUTPUT_LENGTH=1000

# ── Hybrid regex/LLM (langcore-hybrid) ───────────────────────────────────
# Enable hybrid regex+LLM model wrapping (skips LLM for high-confidence patterns)
HYBRID_ENABLED=false
# Minimum regex confidence to bypass LLM (0.0-1.0)
HYBRID_MIN_CONFIDENCE=0.8

# ── DSPy prompt optimization (langcore-dspy) ──────────────────────────────
# Enable DSPy prompt optimization endpoint
DSPY_ENABLED=false
# LLM model for DSPy optimization (needs function calling / tool use)
DSPY_MODEL_ID=gemini/gemini-2.5-flash
# Optimizer strategy: miprov2 (fast, general), gepa (reflective, feedback-driven)
DSPY_OPTIMIZER=miprov2
# Number of candidate prompts to explore (MIPROv2 only)
DSPY_NUM_CANDIDATES=7
# Max bootstrapped/labelled demos for the optimizer
DSPY_MAX_BOOTSTRAPPED_DEMOS=3
DSPY_MAX_LABELED_DEMOS=4
# Thread count for parallel evaluation during optimization
DSPY_NUM_THREADS=4
# Directory for saved optimized configs (save/load persistence)
DSPY_CONFIG_DIR=.dspy_configs

# ── RAG query parsing (langcore-rag) ──────────────────────────────────────
# Enable RAG query parsing endpoint
RAG_ENABLED=false
# LLM model for query decomposition
RAG_MODEL_ID=gpt-4o
# Sampling temperature for the parser (lower = more deterministic)
RAG_TEMPERATURE=0.0
# Max tokens for the parser LLM response
RAG_MAX_TOKENS=1024
# Retry attempts when the LLM returns malformed JSON
RAG_MAX_RETRIES=2
